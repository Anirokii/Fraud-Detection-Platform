"""
Fraud Detection Model Training
===============================
Ce script entra√Æne un mod√®le XGBoost pour d√©tecter les fraudes,
en utilisant SMOTE pour le class imbalance et MLflow pour le tracking.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import pickle
import time
import warnings
warnings.filterwarnings('ignore')
import tempfile
import shutil

# ML Libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    roc_auc_score, precision_recall_curve, average_precision_score,
    confusion_matrix, classification_report, roc_curve
)
from imblearn.over_sampling import SMOTE
import xgboost as xgb

# MLflow
import mlflow
import mlflow.xgboost

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Configuration
BASE_DIR = Path(__file__).parent.parent.parent
DATA_DIR = BASE_DIR / "data" / "processed"
MODELS_DIR = BASE_DIR / "models"
MLFLOW_DIR = BASE_DIR / "mlflow"

# Cr√©er les dossiers
MODELS_DIR.mkdir(exist_ok=True)
MLFLOW_DIR.mkdir(exist_ok=True)

# Configuration MLflow
mlflow.set_tracking_uri(f"file:///{MLFLOW_DIR}")
mlflow.set_experiment("fraud-detection")


class FraudModelTrainer:
    """Classe pour entra√Æner le mod√®le de d√©tection de fraude"""
    
    def __init__(self):
        """Initialisation"""
        print("="*70)
        print("üöÄ FRAUD DETECTION MODEL TRAINING")
        print("="*70)
        
        self.model = None
        self.scaler = None
        self.label_encoders = {}
        self.feature_names = None
        
    def load_data(self):
        """
        √âtape 1: Charger les donn√©es enrichies
        """
        print("\nüìä Chargement des donn√©es...")
        
        data_path = DATA_DIR / "fraud_transactions_enriched.csv"
        self.df = pd.read_csv(data_path)
        
        print(f"‚úÖ {len(self.df):,} transactions charg√©es")
        print(f"   - Fraudes: {self.df['is_fraud'].sum():,} ({self.df['is_fraud'].mean()*100:.3f}%)")
        print(f"   - L√©gitimes: {(~self.df['is_fraud'].astype(bool)).sum():,}")
        
        return self
    
    def prepare_features(self):
        """
        √âtape 2: Pr√©parer les features pour le ML
        
        FEATURES S√âLECTIONN√âES:
        - Temporelles: hour, day_of_week, is_weekend, is_night
        - Comportementales: tx_velocity, amt_deviation, customer_tx_count, days_since_first_tx
        - Cat√©gorielles: category, gender, state
        - Montant: amt
        - Risque: is_high_risk_category
        """
        print("\nüîß Pr√©paration des features...")
        
        # Features num√©riques
        numeric_features = [
            'amt',                    # Montant
            'hour',                   # Heure
            'day_of_week',           # Jour semaine
            'is_weekend',            # Weekend
            'is_night',              # Nuit
            'tx_velocity',           # V√©locit√©
            'amt_deviation',         # D√©viation montant
            'customer_tx_count',     # Nb transactions
            'days_since_first_tx',   # Jours depuis 1√®re tx
            'is_high_risk_category', # Cat√©gorie risque
            'customer_age'           # √Çge client
        ]
        
        # Features cat√©gorielles √† encoder
        categorical_features = [
            'category',  # Cat√©gorie marchand
            'gender',    # Genre client
            'state'      # √âtat
        ]
        
        # Cr√©er le dataset
        X = self.df[numeric_features + categorical_features].copy()
        y = self.df['is_fraud'].values
        
        # G√©rer les valeurs manquantes
        X = X.fillna(0)
        
        # Encoder les variables cat√©gorielles
        print("  üè∑Ô∏è  Encodage des variables cat√©gorielles...")
        for col in categorical_features:
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col].astype(str))
            self.label_encoders[col] = le
        
        # Sauvegarder les noms de features
        self.feature_names = X.columns.tolist()
        
        print(f"‚úÖ {len(self.feature_names)} features pr√©par√©es:")
        for feat in self.feature_names:
            print(f"   ‚Ä¢ {feat}")
        
        self.X = X
        self.y = y
        
        return self
    
    def split_data(self, test_size=0.3, random_state=42):
        """
        √âtape 3: S√©parer train/test
        """
        print(f"\n‚úÇÔ∏è  Split des donn√©es (train: {(1-test_size)*100:.0f}%, test: {test_size*100:.0f}%)...")
        
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X, self.y,
            test_size=test_size,
            random_state=random_state,
            stratify=self.y  # Garde la m√™me proportion de fraudes
        )
        
        print(f"‚úÖ Train: {len(self.X_train):,} samples")
        print(f"   - Fraudes: {self.y_train.sum():,} ({self.y_train.mean()*100:.3f}%)")
        print(f"‚úÖ Test: {len(self.X_test):,} samples")
        print(f"   - Fraudes: {self.y_test.sum():,} ({self.y_test.mean()*100:.3f}%)")
        
        return self
    
    def scale_features(self):
        """
        √âtape 4: Normaliser les features
        
        POURQUOI? Pour que toutes les features soient sur la m√™me √©chelle
        """
        print("\nüìè Normalisation des features...")
        
        self.scaler = StandardScaler()
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        print("‚úÖ Features normalis√©es (mean=0, std=1)")
        
        return self
    
    def apply_smote(self, sampling_strategy=0.5):
        """
        √âtape 5: Appliquer SMOTE pour g√©rer le class imbalance
        
        STRAT√âGIE:
        - sampling_strategy=0.5 = cr√©er des fraudes synth√©tiques jusqu'√† 50% du nb de l√©gitimes
        - Pas 100% car trop de synth√©tiques peut d√©grader la qualit√©
        """
        print(f"\nüîÑ Application de SMOTE (sampling_strategy={sampling_strategy})...")
        
        print(f"  Avant SMOTE:")
        print(f"    L√©gitimes: {(self.y_train == 0).sum():,}")
        print(f"    Fraudes: {(self.y_train == 1).sum():,}")
        
        smote = SMOTE(
            sampling_strategy=sampling_strategy,
            random_state=42,
            k_neighbors=5
        )
        
        self.X_train_resampled, self.y_train_resampled = smote.fit_resample(
            self.X_train_scaled,
            self.y_train
        )
        
        print(f"  Apr√®s SMOTE:")
        print(f"    L√©gitimes: {(self.y_train_resampled == 0).sum():,}")
        print(f"    Fraudes: {(self.y_train_resampled == 1).sum():,}")
        
        fraud_increase = (self.y_train_resampled == 1).sum() / (self.y_train == 1).sum()
        print(f"‚úÖ Fraudes augment√©es de {fraud_increase:.1f}x (synth√©tiques)")
        
        return self
    
    def train_xgboost(self):
        """
        √âtape 6: Entra√Æner XGBoost
        
        HYPERPARAM√àTRES OPTIMIS√âS POUR LA FRAUDE:
        - scale_pos_weight: P√©nalise plus les erreurs sur les fraudes
        - max_depth: Profondeur des arbres (6 = bon √©quilibre)
        - learning_rate: Vitesse d'apprentissage (0.1 = standard)
        - n_estimators: Nombre d'arbres (500 avec early stopping)
        - subsample: 80% des donn√©es par arbre (√©vite overfitting)
        """
        print("\nüéì Entra√Ænement du mod√®le XGBoost...")
        
        # Calculer le poids des classes
        n_legit = (self.y_train == 0).sum()
        n_fraud = (self.y_train == 1).sum()
        scale_pos_weight = n_legit / n_fraud
        
        print(f"  ‚öñÔ∏è  scale_pos_weight = {scale_pos_weight:.1f}")
        
        # Hyperparam√®tres
        params = {
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 500,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'scale_pos_weight': scale_pos_weight,
            'random_state': 42,
            'tree_method': 'hist',  # Plus rapide
            'n_jobs': -1  # Utiliser tous les CPU
        }
        
        # Cr√©er le mod√®le
        self.model = xgb.XGBClassifier(**params)
        
        # Entra√Æner avec early stopping
        print("  üîÑ Entra√Ænement en cours...")
        start_time = time.time()
        
        self.model.fit(
            self.X_train_resampled,
            self.y_train_resampled,
            eval_set=[(self.X_test_scaled, self.y_test)],
            verbose=False  # Pas d'output verbeux
        )
        
        elapsed = time.time() - start_time
        print(f"‚úÖ Mod√®le entra√Æn√© en {elapsed:.1f}s")
        
        # Sauvegarder les params pour MLflow
        self.params = params
        
        return self
    
    def evaluate_model(self):
        """
        √âtape 7: √âvaluer le mod√®le
        """
        print("\nüìä √âvaluation du mod√®le...")
        
        # Pr√©dictions
        y_pred_proba = self.model.predict_proba(self.X_test_scaled)[:, 1]
        y_pred = (y_pred_proba >= 0.5).astype(int)
        
        # M√©triques principales
        roc_auc = roc_auc_score(self.y_test, y_pred_proba)
        avg_precision = average_precision_score(self.y_test, y_pred_proba)
        
        print(f"\nüéØ M√âTRIQUES PRINCIPALES:")
        print(f"  ‚Ä¢ ROC-AUC Score: {roc_auc:.4f}")
        print(f"  ‚Ä¢ Average Precision: {avg_precision:.4f}")
        
        # Confusion Matrix
        cm = confusion_matrix(self.y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        print(f"\nüìà CONFUSION MATRIX:")
        print(f"  True Negatives (TN):  {tn:,}")
        print(f"  False Positives (FP): {fp:,} ‚ö†Ô∏è  (Fausses alertes)")
        print(f"  False Negatives (FN): {fn:,} üö® (Fraudes manqu√©es)")
        print(f"  True Positives (TP):  {tp:,} ‚úÖ (Fraudes d√©tect√©es)")
        
        # M√©triques d√©riv√©es
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        print(f"\nüíØ M√âTRIQUES D√âTAILL√âES:")
        print(f"  ‚Ä¢ Precision: {precision:.4f} (Fiabilit√© des alertes)")
        print(f"  ‚Ä¢ Recall: {recall:.4f} (% de fraudes d√©tect√©es)")
        print(f"  ‚Ä¢ F1-Score: {f1:.4f}")
        
        # Sauvegarder les m√©triques
        self.metrics = {
            'roc_auc': roc_auc,
            'avg_precision': avg_precision,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'true_positives': int(tp)
        }
        
        return self
    
    def log_to_mlflow(self):
        """
        √âtape 8: Logger dans MLflow (version simplifi√©e)
        """
        print("\nüìù Logging dans MLflow...")
        
        try:
            with mlflow.start_run(run_name="xgboost_fraud_detector"):
                # Logger les param√®tres
                mlflow.log_params(self.params)
                
                # Logger les m√©triques
                mlflow.log_metrics(self.metrics)
                
                # Logger les artifacts (features, config)
                mlflow.log_param("features", ", ".join(self.feature_names))
                mlflow.log_param("smote_applied", True)
                mlflow.log_param("scaler", "StandardScaler")
                mlflow.log_param("n_features", len(self.feature_names))
                
                # Sauvegarder le mod√®le comme artifact (pas avec log_model)
                import tempfile
                import shutil
                
                with tempfile.TemporaryDirectory() as tmp_dir:
                    tmp_model_path = Path(tmp_dir) / "model.pkl"
                    with open(tmp_model_path, 'wb') as f:
                        pickle.dump(self.model, f)
                    mlflow.log_artifact(str(tmp_model_path), "model")
                
                print("‚úÖ Run enregistr√©e dans MLflow!")
                print(f"   üìÅ MLflow UI: mlflow ui --backend-store-uri file:///{MLFLOW_DIR}")
        
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: MLflow logging failed: {e}")
            print("   (Le mod√®le est sauvegard√© localement, c'est juste MLflow qui a un probl√®me)")
        
        return self
    
    def save_model(self):
        """
        √âtape 9: Sauvegarder le mod√®le et les artifacts
        """
        print("\nüíæ Sauvegarde du mod√®le...")
        
        # Sauvegarder le mod√®le
        model_path = MODELS_DIR / "fraud_detector_v1.pkl"
        with open(model_path, 'wb') as f:
            pickle.dump(self.model, f)
        print(f"‚úÖ Mod√®le: {model_path}")
        
        # Sauvegarder le scaler
        scaler_path = MODELS_DIR / "scaler.pkl"
        with open(scaler_path, 'wb') as f:
            pickle.dump(self.scaler, f)
        print(f"‚úÖ Scaler: {scaler_path}")
        
        # Sauvegarder les label encoders
        encoders_path = MODELS_DIR / "label_encoders.pkl"
        with open(encoders_path, 'wb') as f:
            pickle.dump(self.label_encoders, f)
        print(f"‚úÖ Encoders: {encoders_path}")
        
        # Sauvegarder les noms de features
        features_path = MODELS_DIR / "feature_names.pkl"
        with open(features_path, 'wb') as f:
            pickle.dump(self.feature_names, f)
        print(f"‚úÖ Feature names: {features_path}")
        
        # Sauvegarder les m√©triques
        metrics_path = MODELS_DIR / "metrics.pkl"
        with open(metrics_path, 'wb') as f:
            pickle.dump(self.metrics, f)
        print(f"‚úÖ Metrics: {metrics_path}")
        
        return self
    
    def plot_feature_importance(self):
        """
        √âtape 10: Visualiser l'importance des features
        """
        print("\nüìä G√©n√©ration du graphique d'importance des features...")
        
        # Obtenir l'importance
        importance = self.model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        # Top 15 features
        top_features = feature_importance_df.head(15)
        
        # Plot
        plt.figure(figsize=(12, 8))
        sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')
        plt.title('Top 15 Most Important Features for Fraud Detection', fontsize=16, fontweight='bold')
        plt.xlabel('Importance Score', fontsize=12)
        plt.ylabel('Feature', fontsize=12)
        plt.tight_layout()
        
        # Sauvegarder
        plot_path = MODELS_DIR / "feature_importance.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ Graphique sauvegard√©: {plot_path}")
        
        print("\nüèÜ TOP 10 FEATURES:")
        for i, row in top_features.head(10).iterrows():
            print(f"  {i+1:2d}. {row['feature']:25s} ‚Üí {row['importance']:.4f}")
        
        return self
    
    def run_full_pipeline(self):
        """
        Ex√©cuter le pipeline complet
        """
        start_time = time.time()
        
        (self
         .load_data()
         .prepare_features()
         .split_data()
         .scale_features()
         .apply_smote()
         .train_xgboost()
         .evaluate_model()
         .log_to_mlflow()
         .save_model()
         .plot_feature_importance())
        
        elapsed = time.time() - start_time
        
        print("\n" + "="*70)
        print(f"‚úÖ PIPELINE TERMIN√â EN {elapsed/60:.1f} MINUTES!")
        print("="*70)
        
        print("\nüéâ R√âCAPITULATIF:")
        print(f"  ‚Ä¢ ROC-AUC: {self.metrics['roc_auc']:.4f}")
        print(f"  ‚Ä¢ Recall: {self.metrics['recall']:.4f}")
        print(f"  ‚Ä¢ Precision: {self.metrics['precision']:.4f}")
        print(f"  ‚Ä¢ Fraudes d√©tect√©es: {self.metrics['true_positives']}/{self.metrics['true_positives'] + self.metrics['false_negatives']}")
        
        print("\nüìÅ FICHIERS CR√â√âS:")
        print(f"  ‚Ä¢ Mod√®le: {MODELS_DIR}/fraud_detector_v1.pkl")
        print(f"  ‚Ä¢ Importance: {MODELS_DIR}/feature_importance.png")
        
        print("\nüîç PROCHAINES √âTAPES:")
        print("  1. Visualiser MLflow UI:")
        print(f"     ‚Üí cd {BASE_DIR}")
        print(f"     ‚Üí mlflow ui --backend-store-uri file:///{MLFLOW_DIR}")
        print("  2. Tester le mod√®le:")
        print("     ‚Üí python src/ml_models/predict_fraud.py")


def main():
    """Point d'entr√©e"""
    trainer = FraudModelTrainer()
    trainer.run_full_pipeline()


if __name__ == "__main__":
    main()